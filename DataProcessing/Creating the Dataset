import pandas as pd  # Import pandas library for data manipulation and analysis.
import os  # Import os library to interact with the operating system for file handling.

# Constants
NUMBER_OF_RAW = 34  # Index of the row to extract GHI data (zero-based indexing).
NUMBER_OF_PREV_TIME = 20  # Number of previous time steps to include, spaced 15 minutes apart.

# file:
csv_directory = 'C:/Users/hadar/Desktop/raw-20241220T103937Z-001/raw'  # Path to the directory containing the CSV files.
output_file = 'C:/Users/hadar/Desktop/nrel_united/processed_data_final_all_pvs.csv'  # Path to save the processed data.

# List to store processed data
processed_data = []  # This list will hold dictionaries, each representing one processed row of data.

# Iterate through all files in the specified directory
for file_name in os.listdir(csv_directory):
    # os.listdir lists all files in the given directory.
    if file_name.endswith('.csv'):
        # Process only files with a .csv extension to avoid errors with other file types.
        file_path = os.path.join(csv_directory, file_name)
        # Combine the directory path with the file name to create the full file path.

        try:
            # Step 1: Read the raw file
            df_raw = pd.read_csv(file_path, header=None, skiprows=1, low_memory=False)
            # Load the file without headers to access metadata and raw data.

            # Step 2: Verify the file format and ensure it contains data for the year 2017
            if df_raw.iloc[5, 0] == '2017':
                # Ensure the file matches the expected format by checking a specific cell.

                # Step 3: Extract metadata from the raw file
                location_id = df_raw.iloc[0, 1]  # Extract location ID (Row 1, Column 2 in the file).
                latitude = df_raw.iloc[0, 5]  # Extract latitude (Row 1, Column 6 in the file).
                longitude = df_raw.iloc[0, 6]  # Extract longitude (Row 1, Column 7 in the file).

                # Step 4: Read the actual data table starting from the proper row
                df_raw.columns = df_raw.iloc[1]  # Set column headers based on the second row in the file.
                df = df_raw.iloc[2:].reset_index(drop=True)
                # Extract data rows starting from Row 3 and reset the index.

                # Step 5: Ensure the GHI column exists in the data
                if 'GHI' in df.columns:
                    # Check if the 'GHI' column is present to avoid processing incomplete files.

                    # Step 6: Extract the target GHI value for the specified row
                    ghi_t = df.iloc[NUMBER_OF_RAW]['GHI'] if NUMBER_OF_RAW < len(df) else None
                    # Extract GHI for the specified row, or assign None if the row doesn't exist.

                    # Step 7: Generate shifted GHI values for previous time steps
                    # Create a list of GHI values from previous time steps using the `shift` function.
                    prev_ghi = []
                    for i in range(1, NUMBER_OF_PREV_TIME + 1):
                        if int(df['Hour'].iloc[NUMBER_OF_RAW-i]) > 4:
                            prev_ghi.append(df['GHI'].shift(i).iloc[NUMBER_OF_RAW])
                        else:
                            break

                    # Step 8: Prepare a dictionary with all features
                    data_features = {
                        'location_id': location_id,  # Add location ID.
                        'latitude': latitude,  # Add latitude.
                        'longitude': longitude,  # Add longitude.
                        'GHI_t': ghi_t,  # Add current GHI value.
                    }

                    # Add GHI_t-1 to GHI_t-n as additional features
                    for i, value in enumerate(prev_ghi, start=1):
                        real_time = 15 * i
                        data_features[f"GHI_t-{real_time} minutes"] = value

                    # Append the dictionary to the list of processed data
                    processed_data.append(data_features)

        except Exception as e:
            # Catch and print any errors during processing to help with debugging.
            print(f"Error processing file {file_name}: {e}")

# Convert the list of processed dictionaries into a DataFrame
processed_df = pd.DataFrame(processed_data)
# Each dictionary becomes a row in the DataFrame, with keys as column names.

# Save the processed DataFrame to a new CSV file
processed_df.to_csv(output_file, index=False)
# Save the data to the specified output file. The `index=False` ensures row indices are not written.
print(f"Processed data saved to {output_file}")

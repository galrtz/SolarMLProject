import torch # Importing the core PyTorch library for deep learning operations
from torch import nn # Importing PyTorch's neural network module to define and manage neural networks
import torch.nn.functional as F # Importing PyTorch's functional API for commonly used functions like activations (e.g., relu, softmax).
from layers import GraphAttentionLayer # Importing the GraphAttentionLayer class (presumably defined elsewhere in the project) that implements the Graph Attention mechanism.

# This defines a custom class GAT that extends the base PyTorch class nn.Module,
# meaning it's a neural network module and can be used for training and inference:

class GAT(nn.Module):

# After the first attention layer:
# after the first Graph Attention Layer (GAT Layer), the output is passed through an activation
# function called ELU. ELU is a nonlinear activation function that is often used in neural networks to 
# add nonlinearity and help the model learn complex relationships in the data.

# After the second attention layer:
# After the second Graph Attention Layer, the output is passed through a softmax activation function.
# Softmax is used to convert raw output values (logits) into a probability distribution across classes.
# In the context of GAT, it's typically used in classification tasks, where the goal is to classify each 
# node into one of several classes.

#__init__: The constructor method used to initialize the GAT model.
# Parameters:
# in_features:
# Number of input features for each node in the graph.

# n_hidden:
# n_hidden is the number of output features per node produced by the first Graph Attention Layer (GAT).
# The first Graph Attention Layer applies a linear transformation to each node's feature vector. 
# This transformation is achieved by multiplying the input features with a weight matrix (W), 
# which is learned during training.
# The transformation process is: h'_i = W*h_i
# The size of the weight matrix ùëäwill be (in_features, n_hidden), 

# n_heads: 
# Number of attention heads in the first GAT layer.
# In a GAT layer, multiple attention heads are used to capture different patterns of relationships between nodes.
# Each attention head has its own weight matrix and learns a different attention mechanism.
# For example, if n_heads = 8, there will be 8 different sets of attention weights.
# Each of the 8 attention heads will generate a feature vector for each node, and the output
# of each attention head will be of size n_hidden (in this case, 64).

# concat: Whether to concatenate the attention heads' outputs (default is False).
# The outputs of all the attention heads are combined:
# If concat=True: The outputs of all the attention heads are concatenated (combined), so if there are 8 attention heads 
# and n_hidden = 64, the resulting feature vector for each node will have a size of 8√ó64=512
# If concat=False: The outputs of the attention heads are averaged, resulting in a feature vector of size n_hidden for each node (in this case, 64).
# The number 64 (or whatever value you choose for n_hidden) is a hyperparameter that determines how many features each node will have after the first attention layer. 
# This is a design choice that balances model complexity and learning capacity.

# dropout: Dropout rate for regularization (default is 0.4).

# leaky_relu_slope: Slope of the Leaky ReLU activation (default is 0.2).

# num_classes: after all attention layers - num_classes in this case should correspond to 1,
# as you're predicting a single continuous value (the GHI at a certain time).

# Number of classes for the classification task (output size).

def __init__(self,
    in_features,
    n_hidden,
    n_heads,
    num_classes,
    concat=False,
    dropout=0.4,
    leaky_relu_slope=0.2):
    
    #  Initializes the parent class (nn.Module) to ensure that the model works correctly with PyTorch's internal
    #  functionality like automatic differentiation, model saving, and others.
    super(GAT, self).__init__()
    
    # Define First Graph Attention Layer
    # Steps in a Graph Attention Layer:
		#	Compute Attention Scores: Each node computes attention scores for its neighbors, often using a shared learned attention mechanism (e.g., a linear transformation of the node features, followed by a LeakyReLU activation and a softmax).
		#	Apply Attention: For each node, its neighbors' features are weighted by the computed attention scores.
		#	Aggregate Features: The weighted neighbor features are aggregated (typically summed or averaged) to get a new feature vector for each node.
		#	Output the Updated Node Features: Finally, the layer returns the updated feature vector for each node, which contains information from both the node itself and its neighbors, with a focus on the most important neighbors as indicated by the attention mechanism.
		# Mathematical Formulation: h'_i =\Sigma _j alpha_{ij} W*h_j
    self.gat1 = GATConv(
    in_features=in_features, out_features=n_hidden, n_heads=n_heads,
    concat=True, dropout=dropout, leaky_relu_slope=leaky_relu_slope
)
		# Define Second Graph Attention Layer
		# in_features=n_hidden: The number of input features to this layer is equal to the output size of the first layer.
		# out_features=1: For regression, this layer has one output feature (predicting a continuous value per node).
		# n_heads=1: Only one attention head is used in this layer.
		# concat=False: This time, no concatenation of attention heads (since there's only one head).
		self.gat2 = GATConv(
    in_features=n_hidden, out_features=1, n_heads=1,
    concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope
)
# The forward method defines how the model processes input data and computes the output. 
def forward(self, input_tensor: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor):
 """
    Performs a forward pass through the GAT.

    Args:
        input_tensor (torch.Tensor): Input tensor representing node features.
        adj_mat (torch.Tensor): Adjacency matrix representing graph structure.

    Returns:
        torch.Tensor: Continuous output tensor after the forward pass (predicted GHI values).
    """
    # Apply the first Graph Attention layer
    # Passes the input tensor and adjacency matrix through the first Graph Attention Layer (gat1).
    # This layer performs the attention operation on the graph.
    x = self.gat1(input_tensor, edge_index, edge_attr)
    
    # Applies the ELU (Exponential Linear Unit) activation function to the output of the first Graph 
    # Attention Layer. ELU is a smooth activation function that helps with learning by allowing negative values.
    x = F.elu(x)  # Apply ELU activation function

    # Apply the second Graph Attention layer

    x = self.gat2(x, edge_index, edge_attr)
    
    # Returns the final output from the second GAT layer.
    # This is the predicted GHI value (continuous) for each node.
    return x 	
